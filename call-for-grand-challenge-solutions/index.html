---
# this is an empty front matter
---
<!DOCTYPE html>
<html lang="en">

<head>
    <title>CALL FOR GRAND CHALLENGE SOLUTIONS</title>
    {% include head-tags.html %}
</head>

<body>

    {% include header.html %}
    {% include slider-regular-page.html title="CALL FOR GRAND CHALLENGE SOLUTIONS" %}

    <section id="teaser-call-for-grand-challenge-solutions">
        <div class="container">
            <div class="row">
                <div class="col-md-8 col-sm-12">
                    <div class="block">                        
                        <!-- <div> -->
                        <!--     <h1>News and Highlights</h1> -->
                        <!--     <p> -->
                        <!--         <li>The online evaluation platform is now open!</li> -->
                        <!--         <li>The online evaluation platform will open on the 15th of February 2025</li> -->
                        <!--         <li>Information on ACM Artifacts Badges is available <a class="text-left" -->
                        <!--                 href="/acm-artifacts/">here</a>.</li> -->
                        <!--     </p> -->
                        <!-- </div> -->                        
                        <div>                            
                            <h1>DEBS 2025: Call for Grand Challenge Solutions</h1>
                            
                            <p>
                                The DEBS Grand Challenge is a series of competitions that started in 2010, in which participants from academia and industry compete with the goal of building faster and more scalable distributed and event-based systems that solve a practical problem. Every year, the DEBS Grand Challenge participants have a chance to explore a new data set and a new problem and can compare their results based on common evaluation criteria. The winners of the challenge are announced during the conference. Apart from correctness and performance, submitted solutions are also assessed along a set of non-functional requirements.
                            </p>

                            <!-- <p><b>Registration is Open!</b> <a href="https://cmt3.research.microsoft.com/DEBS2025/Track/3/Submission/Create">https://cmt3.research.microsoft.com/DEBS2025/Track/3/Submission/Create</a></p> -->

<!--                             <h1> -->
<!--                                 Topic: Defect Monitoring in Additive Manufacturing -->
<!--                             </h1> -->

<!--                             <p> -->
<!--                                 The 2025 DEBS Grand Challenge focuses on real-time monitoring of defects in Laser Powder Bed Fusion (L-PBF) manufacturing. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 L-PBF is an additive manufacturing method, meaning that it creates objects by adding material rather than removing parts from an initial block, which reduces waste and increases flexibility. Specifically, L-PBF melts layers of fine metal powder on top of each other, according to a specification (usually, a CAD file). <br> -->

<!--                                 <img src="/img/gc/image-asset.gif"> <br> -->

<!--                                 Image source: <a href="http://pencerw.com/feed/2014/12/4/island-scanning-and-the-effects-of-slm-scanning-strategies">http://pencerw.com/feed/2014/12/4/island-scanning-and-the-effects-of-slm-scanning-strategies</a> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Unfortunately, L-PBF is not immune to defects due to impurities in the material used or calibration errors. In particular, a high degree of porosity (amount of empty spaces left inside the material) may severely impact the durability of the object being created. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The established approach to assess the quality of the process is through the analysis of the object after it is fully created. However, this approach leads to a waste of time and material, as the analysis is carried out afterward. To overcome these problems, more recent studies propose techniques that monitor the creation process in real-time, blocking a printing process if there is a high probability of defects. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The goal of the 2025 DEBS Grand Challenge is to implement a real-time monitoring system to detect defects in L-PBF manufacturing. The system receives as input a stream of optical tomography images, indicating the temperature of the powder bed for each layer during the manufacturing process. It must compute the probability that the object being created presents defects in some areas, enabling rapid reactions in the case of problems. -->
<!--                             </p> -->

<!--                         </div> -->
<!--                         <div> -->
<!--                             <h1>Participation</h1> -->

<!--                             <p> -->
<!--                                 Participation in the DEBS 2025 Grand Challenge consists of three steps: (1) registration, (2) iterative solution submission, and (3) paper submission. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The first step is to register your submission in the "grand challenge track”. Shortly after your registration, you will have access to a containerized environment to test your solution directly on your computer, and a sample naïve solution to compare your approach with. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Once your solution is ready, you can submit it to the evaluation platform (<a href="https://challenge2025.debs.org/">https://challenge2025.debs.org/</a>) to get benchmarked in the challenge. The evaluation platform provides detailed feedback on performance and allows the solution to be updated in an iterative process. A solution can be continuously improved until the challenge closing date. Evaluation results of the last submitted solution will be used for the final performance ranking. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The last step is to upload a short paper (maximum 6 pages) describing the final solution via the conference management tool. The DEBS Grand Challenge Committee will review all papers to assess the merit and originality of submitted solutions. Participants will have the opportunity to present their work during the poster session at the DEBS 2025 conference. -->
<!--                             </p> -->

<!--                         </div> -->
<!--                         <div> -->
                            
<!--                             <h1>Query</h1> -->

<!--                             <p> -->
<!--                                 The 2025 DEBS Grand Challenge involves implementing a monitoring system that takes in input a stream of optical tomography images of an object being created, and outputs a prediction that the object presents defects in some areas. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Each optical tomography image contains, for each point P = (x, y), the temperature T(P) at that point, measured layer by layer (z). -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Images are submitted as a continuous stream, simulating the monitoring of the object while it is being printed. The stream delivers images one layer at a time, and each layer is split into multiple blocks, which we call tiles. See the figure below for reference. <br> -->

<!--                                 <img src="/img/gc/tiles.png"> -->
<!--                             </p> -->
                            
<!--                             <p> -->
<!--                                 The processing pipeline involves 4 steps: -->
<!--                                 <ol> -->
<!--                                     <li><b>Saturation analysis.</b> Within each tile, detect all points that surpass a threshold value of 65000.</li> -->
<!--                                     <li><b>Windowing.</b> For each tile, keep a window of the last three layers.</li> -->
<!--                                     <li><b>Outlier analysis.</b> Within each tile window, for each point P of the most recent layer, compute its local temperature deviation as difference between the mean temperature T(P) of its close neighbors (Manhattan distance 0 <= d <= 2 considering the 3 layers stacked on top of each other) and that of its outer neighbours (Manhattan distance 2 < d <=4) in absolute value. Let us call this value D. We define an outlier as a point for which D > 5000.</li> -->
<!--                                     <li><b>Outlier clustering.</b> Using the outliers computed for the last received layer, find clusters of nearby outliers using DBScan, considering the Euclidean distance between points as the distance metric.</li> -->
<!--                                 </ol> -->
<!--                             </p> -->


<!--                             <p> -->
<!--                                 For each input (tile) received from the stream, your solution should return: -->
<!--                                 <ol> -->
<!--                                     <li>The number of saturated points.</li> -->
<!--                                     <li>The centroid (x, y coordinates) and size (number of points) of the top 10 largest clusters.</li> -->
<!--                                 </ol> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The figure below illustrates how windows are computed. <br> -->

<!--                                 <img src="/img/gc/windows.png"> -->
<!--                             </p> -->
                            
<!--                             <p> -->
<!--                                 As a concrete example of the process, the image below shows an image extracted from the input dataset. <br> -->

<!--                                 <img src="/img/gc/example.jpg"> -->
<!--                             </p> -->
                            
<!--                             <p> -->
<!--                                 The analysis of saturation identifies over 5000 saturated points, corresponding to white areas (e.g., the area in the top right triangle). -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The analysis of outliers identifies the points as shown in the figure below. <br> -->

<!--                                 <img src="/img/gc/analysis.png"> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Finally, the cluster analysis identifies the following clusters: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li>Cluster 6: Centroid (498.1222272521503, 1633.0054323223178), Size 2209</li> -->
<!--                                     <li>Cluster 5: Centroid (492.52658371040724, 1168.7143665158371), Size 1768</li> -->
<!--                                     <li>Cluster 0: Centroid (343.49514563106794, 1624.8543689320388), Size 309</li> -->
<!--                                     <li>Cluster 30: Centroid (1147.0236966824646, 1011.8388625592418), Size 211</li> -->
<!--                                     <li>Cluster 7: Centroid (463.44827586206895, 1247.603448275862), Size 174</li> -->
<!--                                     <li>Cluster 29: Centroid (1148.624060150376, 953.9774436090225), Size 133</li> -->
<!--                                     <li>Cluster 32: Centroid (1267.4313725490197, 1032.4705882352941), Size 102</li> -->
<!--                                     <li>Cluster 28: Centroid (1070.6382978723404, 981.3829787234042), Size 94</li> -->
<!--                                     <li>Cluster 8: Centroid (458.10752688172045, 1212.6989247311828), Size 93</li> -->
<!--                                     <li>Cluster 19: Centroid (893.3134328358209, 586.9701492537314), Size 67</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                         </div> -->
<!--                         <div> -->

<!--                             <h1>Challenger 2.0 and Suggested Workflow</h1> -->
                            
<!--                             <p> -->
<!--                                 We have been using Challenger to disseminate the dataset and benchmark solutions [1, 2, 3]. This year, we introduce <a href="https://challenge2025.debs.org/">Challenger 2.0</a>, which switches to REST API instead of RPC calls. Furthermore, it uses a containerized evaluation infrastructure [4]. Challenger provides a REST API, which can accessed at <a href="http://challenge2025.debs.org:52923/api">http://challenge2025.debs.org:52923/api</a>. The OpenAPI specification of the REST API is explained below. The API provides 5 endpoints to benchmark your solutions. -->
<!--                                 <ol start="1"> -->
<!--                                     <li>/create (Creates a new benchmark)</li> -->
<!--                                     <li>/start (Starts the benchmark)</li> -->
<!--                                     <li>/next_batch (Get the next batch of data)</li> -->
<!--                                     <li>/result (Submit results, go to Step 2, repeat until there is no more data)</li> -->
<!--                                     <li>/end (End the benchmark)</li>                                -->
<!--                                 </ol> -->
<!--                                 Please use Challenger to develop your solutions. We provide a local evaluator platform and a <a href="https://github.com/chrido/bandency/blob/challenger2/query/2025/short_client.py">starter code to</a> help start your development. In addition, we provide a naive <a href="https://polimi365-my.sharepoint.com/:f:/g/personal/10565376_polimi_it/EvwDVD1Etp1LijxzcmVt4fEBzF90wUk3KxEgQAAn46DOmA?e=gfw3TP">reference implementation</a> of the problem, which we use as a baseline for all submissions. This solution is not performant but provides a simple reference to verify the correct understanding of the challenge requirements. -->
<!--                             </p> -->

<!--                             <\!--p> -->
<!--                                 The Challenger defines the following API endpoints that enable the creation, execution, and processing of benchmarking sessions. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 The workflow proceeds as follows: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li>Clients <i>create</i> and <i>start</i> benchmarks (bench for short);</li> -->
<!--                                     <li>Within each benchmark, they request <i>batches</i> of data;</li> -->
<!--                                     <li>For each batch, they provide a <i>result</i> for the queries;</li> -->
<!--                                     <li>After submitting results for all the batches of data, they <i>end</i> a benchmark.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 In the local challenger, clients can visualize the statistics of their benchmarks (throughput and latency percentiles) through a Web interface through the <i>history</i> endpoint. -->
<!--                             </p-\-> -->

<!--                             <h2>Containerized evaluation infrastructure</h2> -->

<!--                             <p> -->
<!--                                 To foster the culture of cloud-native solutions in the community and ensure the reproducibility of the solutions, we provide a Kubernetes-cluster-based evaluation infrastructure. Please containerize your solution and develop a Kubernetes <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">job workload</a> for solution deployment. Please upload the job YAML file to Challenger so that your solutions can be deployed on the cluster. Please run your final evaluations on Challenger infrastructure. Challenger utilizes <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespaces</a> to sandbox solutions. Furthermore, it uses <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuotas</a> and <a href="https://kubernetes.io/docs/concepts/policy/limit-range/">LimitRanges</a> to provide uniform resources to each namespace to ensure a fair evaluation. Each namespace is assigned 4 CPU cores and 8 GBs of memory. You can distribute these resources among pods. By default, each container is given 1 CPU core and 2 GBs of memory. -->
<!--                             </p> -->

<!--                             <h2>Fault-tolerance evaluations</h2> -->

<!--                             <p> -->
<!--                                 Faults are a common occurrence in any infrastructure, and faults may drastically reduce the performance of a solution [5]. A practical solution should not only be performant but also fault-tolerant. Challenger provides fault-tolerance evaluations to measure the solutions' performance under failure. Challenger can simulate processes (pods) and network failures. Pods failures make a process unavailable, and network failures can introduce network delays in the network of the solution. Please read Section 3.3 of [5] on the discussion to choose these failures. Please run your solutions against both faults with varying frequencies and report your results. -->
<!--                             </p> -->

<!--                             <h2>Micro Challenger</h2> -->

<!--                             <p> -->
<!--                                 To accelerate the speed of local development and remove network bandwidth constraints, we additionally provide Micro Challenger to run on your own systems. <a href="https://polimi365-my.sharepoint.com/:f:/g/personal/10565376_polimi_it/EvwDVD1Etp1LijxzcmVt4fEBzF90wUk3KxEgQAAn46DOmA?e=gfw3TP">Micro Challenger</a> uses the same endpoints as Challenger and is cross-compatible. In Micro Challenger, participants can visualize the statistics of their benchmarks (throughput and latency percentiles) through a Web interface through the history endpoint. -->
<!--                             </p> -->
                            
<!--                             <h2>REST Endpoints</h2> -->

<!--                             <p> -->
<!--                                 Here is a detailed description of the available endpoints: -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <ol start="1"> -->
<!--                                 <li><b>POST</b> <span style="color: green;">/api/create</span></li> -->
<!--                             </ol> -->

<!--                             <p> -->
<!--                                 <b>Description</b>: Creates a new "bench" or task with a set of parameters. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Input JSON</b>: -->

<!--                                 <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{ -->
<!--     "user_id": "id_of_user", -->
<!--     "name": "name_of_bench", -->
<!--     "test": true, -->
<!--     "queries": [0], -->
<!--     "max_batches": &lt;limit&gt; -->
<!-- }</code></pre> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Where -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li><span style="color: green;">user_id</span>: Identifier for the user.</li> -->
<!--                                     <li><span style="color: green;">name</span>: The name of the bench.</li> -->
<!--                                     <li><span style="color: green;">test</span>: A boolean to specify if this is a test (test runs will not be considered in the online platform)</li> -->
<!--                                     <li><span style="color: green;">queries</span>: A list of queries to be executed (always [0] for this year's challenge)</li> -->
<!--                                     <li><span style="color: green;">max_batches</span>: The maximum number of batches to process (optional).</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>: Returns the <span style="color: green;">bench_id</span> of the created task. -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <ol start="2"> -->
<!--                                 <li><b>POST</b> <span style="color: green;">/api/start/{bench_id}</span></li> -->
<!--                             </ol> -->

<!--                             <p> -->
<!--                                 <b>Description</b>: Starts the execution of the bench identified by <span style="color: green;">bench_id</span>. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>URL Parameter:</b> -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li><span style="color: green;">bench_id</span>: The identifier of the bench to be started.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>: Returns status code <span style="color: green;">200</span> on successful start. -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <ol start="3"> -->
<!--                                 <li><b>GET</b> <span style="color: green;">/api/next_batch/{bench_id}</span></li> -->
<!--                             </ol> -->

<!--                             <p> -->
<!--                                 <b>Description</b>: Retrieves the next batch of data for the specified bench. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>URL Parameter</b>: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li>On success: Returns a batch of data encoded in MessagePack (<a href="https://msgpack.org/">https://msgpack.org/</a>).</li> -->
<!--                                     <li>On completion: Returns status code <span style="color: green;">404</span> when there are no more batches available.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <ol start="4"> -->
<!--                                 <li><b>POST</b> <span style="color: green;">/api/result/0/{bench_id}/{batch_number}</span></li> -->
<!--                             </ol> -->

<!--                             <p> -->
<!--                                 <b>Description</b>: Submits the result of processing a batch. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>URL Parameters</b>: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li> -->
<!--                                     <li><span style="color: green;">bench_number</span>: The number of the batch being submitted.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

                            
<!--                             <p> -->
<!--                                 <b>Data</b>: The processed result serialized in MessagePack (<a href="https://msgpack.org/">https://msgpack.org/</a>). -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>: Returns status code <span style="color: green;">200</span> on successful start. -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <ol start="5"> -->
<!--                                 <li><b>POST</b> <span style="color: green;">/api/end/{bench_id}</span></li> -->
<!--                             </ol> -->

<!--                             <p> -->
<!--                                 <b>Description</b>: Marks the bench as completed. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>URL Parameter</b>: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>: Returns the final result as a string upon successful completion. -->
<!--                             </p> -->

<!--                             <hr> -->

<!--                             <p> -->
<!--                                 <b>Batch Input</b>. Each batch contains data such as: -->

<!--                                 <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{ -->
<!--     "sequence": &lt;int&gt;, -->
<!--     "print_id": &lt;int&gt;, -->
<!--     "tile_id": &lt;int&gt;, -->
<!--     "layer": &lt;int&gt;, -->
<!--     "tif": &lt;binary data&gt;                                         -->
<!-- }</code></pre> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Where: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li>sequence: sequence number that uniquely identifies a batch;</li> -->
<!--                                     <li>print_id: identifier of the object being printed;</li> -->
<!--                                     <li>tile_id: identifier of the tile (see the figure above for reference);</li> -->
<!--                                     <li>layer: identifier of the layer (z position) of the tile (see the figure above for reference);</li> -->
<!--                                     <li>tif: the image in tif format.</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 <b>Response</b>. Each response contains data such as: -->

<!--                                 <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{ -->
<!--     "sequence": &lt;int&gt;, -->
<!--     “query”: 0 -->
<!--     "print_id": &lt;int&gt;, -->
<!--     "tile_id": &lt;int&gt;, -->
<!--     "saturated": &lt;int&gt;, -->
<!--     "centroids": [C0, C1, …]                                    -->
<!-- }</code></pre> -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 Where: -->
<!--                                 <ul class="nested-list"> -->
<!--                                     <li>sequence: sequence number that uniquely identifies a batch;</li> -->
<!--                                     <li>print_id: identifier of the object being printed;</li> -->
<!--                                     <li>tile_id: identifier of the tile;</li> -->
<!--                                     <li>saturated: number of saturated points;</li> -->
<!--                                     <li>centroids: list of centroids, where each centroid is defined by its <i>coordinates</i> (x, y) and its <i>size</i> (the number of points belonging to its cluster)</li> -->
<!--                                 </ul> -->
<!--                             </p> -->

<!--                         </div> -->
<!--                         <div> -->

<!--                             <h1>Awards and Selection Process</h1> -->

<!--                             <p> -->
<!--                                 Participants of the challenge compete for the performance award. The winner is determined through the automated evaluation platform Challenger (see above), according to criteria of performance (throughput and latency) and fault tolerance. In the case of submissions with similar performance characteristics, the committees will evaluate qualities of the solutions that are not tied to performance. Specifically, participants are encouraged to consider the practicability of the solution. -->
<!--                             </p> -->
<!--                             <p> -->
<!--                                 Regarding the practicability of a solution, we encourage participants to push beyond solving a problem correctly only in the functional sense. Instead, the aim is to provide a reusable and extensible solution that is of value beyond this year’s Grand Challenge alone, using available software systems or home-made but general-purpose processing engines. -->
<!--                             </p> -->
<!--                             <p> -->
<!--                                 In particular, we will reward those solutions that adhere to a list of non-functional criteria, such as configurability, scalability (horizontal scalability is preferred), operational reliability/resilience, accessibility of the solution's source code, integration with standard (tools/protocols), documentation, security mechanisms, deployment support, portability/maintainability, and support of special hardware (e.g., FPGAs, GPUs, SDNs, ...). These criteria are driven by industry use cases and scenarios so that we make sure solutions are applicable in practical settings. -->
<!--                             </p> -->

<!--                             <p> -->
<!--                                 References: -->
<!--                                 <ol start="1"> -->
<!--                                     <li>The DEBS 2021 grand challenge: analyzing environmental impact of worldwide lockdowns. Tahir et al. DEBS ‘21. <a href="https://doi.org/10.1145/3465480.3467836">https://doi.org/10.1145/3465480.3467836</a></li> -->
<!--                                     <li>Detecting trading trends in financial tick data: the DEBS 2022 grand challenge. Frischbier et al. DEBS ‘22. <a href="https://doi.org/10.1145/3524860.3539645">https://doi.org/10.1145/3524860.3539645</a></li> -->
<!--                                     <li>The DEBS 2024 Grand Challenge: Telemetry Data for Hard Drive Failure Prediction. De Martini et al. DEBS ‘24. <a href="https://doi.org/10.1145/3629104.3672538">https://doi.org/10.1145/3629104.3672538</a></li> -->
<!--                                     <li>Challenger 2.0: A Step Towards Automated Deployments and Resilient Solutions for the DEBS Grand Challenge. Tahir et al. DEBS ‘24. <a href="https://doi.org/10.1145/3629104.3666027">https://doi.org/10.1145/3629104.3666027</a></li> -->
<!--                                     <li>How Reliable Are Streams? End-to-End Processing-Guarantee Validation and Performance Benchmarking of Stream Processing Systems. Tahir et al. PVLDB ‘24. https://doi.org/10.14778/3712221.3712227</li> -->
<!--                                 </ol> -->
                                
<!--                             </p> -->
                            
<!--                         </div> -->
<!--                         <div> -->

<!--                             <h1>Important Dates</h1> -->
<!--                             <\!-- <li> -->
<!--                                 Release of the challenge, initial data set: <b>November 6th, 2023</b> -->
<!--                             </li> -\-> -->
<!--                             <li> -->
<!--                                 Registration and download of local test environment: <b>December, 2024</b> -->
<!--                             </li> -->
<!--                             <li> -->
<!--                                 Online evaluation platform opens: <b>February 15th, 2025</b> -->
<!--                             </li> -->
<!--                             <li> -->
<!--                                 Deadline for short paper (max 6 pages) with design/implementation details (notification follows fast review): <b>May 2nd, 2025</b> -->
<!--                             </li> -->
<!--                             <li> -->
<!--                                 Deadline for Camera Ready submission: <b>May 9th, 2025</b> -->
<!--                             </li> -->
<!--                             <li> -->
<!--                                 Online evaluation platform closes: <b>May 9th, 2025</b> -->
<!--                             </li> -->
                        </div>

                        <div>
                            {% if site.data.chairs.grand_challenge_chairs %}
                            <h1>Grand Challenge Co-Chairs</h1>
                            {% for chair in site.data.chairs.grand_challenge_chairs %}
                            <li>{{ chair.firstname }} {{ chair.lastname }}, {{ chair.institution }}</li>
                            {% endfor %}
                            {% endif %}
                        </div>

                        <div>
                            <h1>ACM Policies and Procedures</h1>
                           <p>
                               By submitting your article to an ACM Publication, you are hereby acknowledging that you 
                               and your co-authors are subject to all <a href="https://www.acm.org/publications/policies">ACM Publications Policies</a>, 
                               including <a href="https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects">ACM's 
                                   Publications Policy on Research Involving Human Participants and Subjects</a>. ACM will investigate alleged 
                               violations of this policy or any ACM Publications Policy and may result in a full retraction 
                               of your paper, in addition to other potential penalties, as per ACM Publications Policy.
                           </p>
                           <p>
                               Regarding <a href="https://www.acm.org/publications/policies/new-acm-policy-on-authorship">ACM Policy on Authorship</a>, 
                               which also provides policies on the use of generative AI tools in preparing manuscripts, please check the 
                               <a href="https://www.acm.org/publications/policies/frequently-asked-questions">frequently asked questions</a> page.
                           </p>
                           <p class="justfy">
                               Please ensure that you and your co-authors obtain <a href="https://authors.acm.org/author-resources/orcid-faqs">an ORCID ID</a>, 
                               so you can complete the publishing process for your accepted paper.
                           </p>
                       </div>

                    </div>
                </div>

                {% include side.html %}
            </div>
    </section>

    {% include footer.html %}
</body>
</html>
