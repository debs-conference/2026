---
# this is an empty front matter
---
<!DOCTYPE html>
<html lang="en">

<head>
    <title>CALL FOR GRAND CHALLENGE SOLUTIONS</title>
    {% include head-tags.html %}
</head>

<body>

    {% include header.html %}
    {% include slider-regular-page.html title="CALL FOR GRAND CHALLENGE SOLUTIONS" %}

    <section id="teaser-call-for-grand-challenge-solutions">
        <div class="container">
            <div class="row">
                <div class="col-md-8 col-sm-12">
                    <div class="block">
                        <div>
                            
                            <h1>DEBS 2025: Call for Grand Challenge Solutions</h1>
                            
                            <p>
                                The DEBS Grand Challenge is a series of competitions that started in 2010, in which participants from academia and industry compete with the goal of building faster and more scalable distributed and event-based systems that solve a practical problem. Every year, the DEBS Grand Challenge participants have a chance to explore a new data set and a new problem and can compare their results based on common evaluation criteria. The winners of the challenge are announced during the conference. Apart from correctness and performance, submitted solutions are also assessed along a set of non-functional requirements.
                            </p>

                            <p><b>Registration is Open!</b> <a href="https://cmt3.research.microsoft.com/DEBS2025/Track/3/Submission/Create">https://cmt3.research.microsoft.com/DEBS2025/Track/3/Submission/Create</a></p>

                            <h1>
                                Topic: Defect Monitoring in Additive Manufacturing
                            </h1>

                            <p>
                                The 2025 DEBS Grand Challenge focuses on real-time monitoring of defects in Laser Powder Bed Fusion (L-PBF) manufacturing.
                            </p>

                            <p>
                                L-PBF is an additive manufacturing method, meaning that it creates objects by adding material rather than removing parts from an initial block, which reduces waste and increases flexibility. Specifically, L-PBF melts layers of fine metal powder on top of each other, according to a specification (usually, a CAD file). <br>

                                <img src="/img/gc/image-asset.gif"> <br>

                                Image source: <a href="http://pencerw.com/feed/2014/12/4/island-scanning-and-the-effects-of-slm-scanning-strategies">http://pencerw.com/feed/2014/12/4/island-scanning-and-the-effects-of-slm-scanning-strategies</a>
                            </p>

                            <p>
                                Unfortunately, L-PBF is not immune to defects due to impurities in the material used or calibration errors. In particular, a high degree of porosity (amount of empty spaces left inside the material) may severely impact the durability of the object being created.
                            </p>

                            <p>
                                The established approach to assess the quality of the process is through the analysis of the object after it is fully created. However, this approach leads to a waste of time and material, as the analysis is carried out afterward. To overcome these problems, more recent studies propose techniques that monitor the creation process in real-time, blocking a printing process if there is a high probability of defects.
                            </p>

                            <p>
                                The goal of the 2025 DEBS Grand Challenge is to implement a real-time monitoring system to detect defects in L-PBF manufacturing. The system receives as input a stream of optical tomography images, indicating the temperature of the powder bed for each layer during the manufacturing process. It must compute the probability that the object being created presents defects in some areas, enabling rapid reactions in the case of problems.
                            </p>

                        </div>
                        <div>
                            <h1>Participation</h1>

                            <p>
                                Participation in the DEBS 2025 Grand Challenge consists of three steps: (1) registration, (2) iterative solution submission, and (3) paper submission.
                            </p>

                            <p>
                                The first step is to register your submission in the "grand challenge track”. Shortly after your registration, you will have access to a containerized environment to test your solution directly on your computer, and a sample naïve solution to compare your approach with.
                            </p>

                            <p>
                                Once your solution is ready, you can submit it to the evaluation platform (<a href="https://challenge2025.debs.org/">https://challenge2025.debs.org/</a>) to get benchmarked in the challenge. The evaluation platform provides detailed feedback on performance and allows the solution to be updated in an iterative process. A solution can be continuously improved until the challenge closing date. Evaluation results of the last submitted solution will be used for the final performance ranking.
                            </p>

                            <p>
                                The last step is to upload a short paper (maximum 6 pages) describing the final solution via the conference management tool. The DEBS Grand Challenge Committee will review all papers to assess the merit and originality of submitted solutions. Participants will have the opportunity to present their work during the poster session at the DEBS 2025 conference.
                            </p>

                        </div>
                        <div>
                            
                            <h1>Query</h1>

                            <p>
                                The 2025 DEBS Grand Challenge involves implementing a monitoring system that takes in input a stream of optical tomography images of an object being created, and outputs a prediction that the object presents defects in some areas.
                            </p>

                            <p>
                                Each optical tomography image contains, for each point P = (x, y), the temperature T(P) at that point, measured layer by layer (z).
                            </p>

                            <p>
                                Images are submitted as a continuous stream, simulating the monitoring of the object while it is being printed. The stream delivers images one layer at a time, and each layer is split into multiple blocks, which we call tiles. See the figure below for reference. <br>

                                <img src="/img/gc/tiles.png">
                            </p>
                            
                            <p>
                                The processing pipeline involves 4 steps:
                                <ol>
                                    <li><b>Saturation analysis.</b> Within each tile, detect all points that surpass a threshold value of 65000.</li>
                                    <li><b>Windowing.</b> For each tile, keep a window of the last three layers.</li>
                                    <li><b>Outlier analysis.</b> Within each tile window, for each point P of the most recent layer, compute its local temperature deviation as difference between the mean temperature T(P) of its close neighbors (Manhattan distance 0 <= d <= 2 considering the 3 layers stacked on top of each other) and that of its outer neighbours (Manhattan distance 2 < d <=4) in absolute value. Let us call this value D. We define an outlier as a point for which D > 5000.</li>
                                    <li><b>Outlier clustering.</b> Using the outliers computed for the last received layer, find clusters of nearby outliers using DBScan, considering the Euclidean distance between points as the distance metric.</li>
                                </ol>
                            </p>


                            <p>
                                For each input (tile) received from the stream, your solution should return:
                                <ol>
                                    <li>The number of saturated points.</li>
                                    <li>The centroid (x, y coordinates) and size (number of points) of the top 10 largest clusters.</li>
                                </ol>
                            </p>

                            <p>
                                The figure below illustrates how windows are computed. <br>

                                <img src="/img/gc/windows.png">
                            </p>
                            
                            <p>
                                As a concrete example of the process, the image below shows an image extracted from the input dataset. <br>

                                <img src="/img/gc/example.jpg">
                            </p>
                            
                            <p>
                                The analysis of saturation identifies over 5000 saturated points, corresponding to white areas (e.g., the area in the top right triangle).
                            </p>

                            <p>
                                The analysis of outliers identifies the points as shown in the figure below. <br>

                                <img src="/img/gc/analysis.png">
                            </p>

                            <p>
                                Finally, the cluster analysis identifies the following clusters:
                                <ul class="nested-list">
                                    <li>Cluster 6: Centroid (498.1222272521503, 1633.0054323223178), Size 2209</li>
                                    <li>Cluster 5: Centroid (492.52658371040724, 1168.7143665158371), Size 1768</li>
                                    <li>Cluster 0: Centroid (343.49514563106794, 1624.8543689320388), Size 309</li>
                                    <li>Cluster 30: Centroid (1147.0236966824646, 1011.8388625592418), Size 211</li>
                                    <li>Cluster 7: Centroid (463.44827586206895, 1247.603448275862), Size 174</li>
                                    <li>Cluster 29: Centroid (1148.624060150376, 953.9774436090225), Size 133</li>
                                    <li>Cluster 32: Centroid (1267.4313725490197, 1032.4705882352941), Size 102</li>
                                    <li>Cluster 28: Centroid (1070.6382978723404, 981.3829787234042), Size 94</li>
                                    <li>Cluster 8: Centroid (458.10752688172045, 1212.6989247311828), Size 93</li>
                                    <li>Cluster 19: Centroid (893.3134328358209, 586.9701492537314), Size 67</li>
                                </ul>
                            </p>

                        </div>
                        <div>

                            <h1>Evaluation Workflow and API</h1>
                            
                            <p>
                                To evaluate their solutions, participants interact with a server denoted as the Challenger through REST API. The final performance evaluation will be conducted in a controlled environment, but after the registration, participants will be provided with a dockerized version of the Challenger that they can use to experiment their solutions. Participants will also receive a naive solution of the challenge written in Python: this solution is not performant but provides a simple reference to verify the correct understanding of the challenge requirements.
                            </p>

                            <p>
                                The Challenger defines the following API endpoints that enable the creation, execution, and processing of benchmarking sessions.
                            </p>

                            <p>
                                The workflow proceeds as follows:
                                <ul class="nested-list">
                                    <li>Clients <i>create</i> and <i>start</i> benchmarks (bench for short);</li>
                                    <li>Within each benchmark, they request <i>batches</i> of data;</li>
                                    <li>For each batch, they provide a <i>result</i> for the queries;</li>
                                    <li>After submitting results for all the batches of data, they <i>end</i> a benchmark.</li>
                                </ul>
                            </p>

                            <p>
                                In the local challenger, clients can visualize the statistics of their benchmarks (throughput and latency percentiles) through a Web interface through the <i>history</i> endpoint.
                            </p>

                            <p>
                                Here is a detailed description of the available endpoints:
                            </p>

                            <hr>

                            <ol start="1">
                                <li><b>POST</b> <span style="color: green;">/api/create</span></li>
                            </ol>

                            <p>
                                <b>Description</b>: Creates a new "bench" or task with a set of parameters.
                            </p>

                            <p>
                                <b>Input JSON</b>:

                                <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{
    "user_id": "id_of_user",
    "name": "name_of_bench",
    "test": true,
    "queries": [0],
    "max_batches": &lt;limit&gt;
}</code></pre>
                            </p>

                            <p>
                                Where
                                <ul class="nested-list">
                                    <li><span style="color: green;">user_id</span>: Identifier for the user.</li>
                                    <li><span style="color: green;">name</span>: The name of the bench.</li>
                                    <li><span style="color: green;">test</span>: A boolean to specify if this is a test (test runs will not be considered in the online platform)</li>
                                    <li><span style="color: green;">queries</span>: A list of queries to be executed (always [0] for this year's challenge)</li>
                                    <li><span style="color: green;">max_batches</span>: The maximum number of batches to process (optional).</li>
                                </ul>
                            </p>

                            <p>
                                <b>Response</b>: Returns the <span style="color: green;">bench_id</span> of the created task.
                            </p>

                            <hr>

                            <ol start="2">
                                <li><b>POST</b> <span style="color: green;">/api/start/{bench_id}</span></li>
                            </ol>

                            <p>
                                <b>Description</b>: Starts the execution of the bench identified by <span style="color: green;">bench_id</span>.
                            </p>

                            <p>
                                <b>URL Parameter:</b>
                                <ul class="nested-list">
                                    <li><span style="color: green;">bench_id</span>: The identifier of the bench to be started.</li>
                                </ul>
                            </p>

                            <p>
                                <b>Response</b>: Returns status code <span style="color: green;">200</span> on successful start.
                            </p>

                            <hr>

                            <ol start="3">
                                <li><b>GET</b> <span style="color: green;">/api/next_batch/{bench_id}</span></li>
                            </ol>

                            <p>
                                <b>Description</b>: Retrieves the next batch of data for the specified bench.
                            </p>

                            <p>
                                <b>URL Parameter</b>:
                                <ul class="nested-list">
                                    <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li>
                                </ul>
                            </p>

                            <p>
                                <b>Response</b>:
                                <ul class="nested-list">
                                    <li>On success: Returns a batch of data encoded in MessagePack (<a href="https://msgpack.org/">https://msgpack.org/</a>).</li>
                                    <li>On completion: Returns status code <span style="color: green;">404</span> when there are no more batches available.</li>
                                </ul>
                            </p>

                            <hr>

                            <ol start="4">
                                <li><b>POST</b> <span style="color: green;">/api/result/0/{bench_id}/{batch_number}</span></li>
                            </ol>

                            <p>
                                <b>Description</b>: Submits the result of processing a batch.
                            </p>

                            <p>
                                <b>URL Parameters</b>:
                                <ul class="nested-list">
                                    <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li>
                                    <li><span style="color: green;">bench_number</span>: The number of the batch being submitted.</li>
                                </ul>
                            </p>

                            
                            <p>
                                <b>Data</b>: The processed result serialized in MessagePack (<a href="https://msgpack.org/">https://msgpack.org/</a>).
                            </p>

                            <p>
                                <b>Response</b>: Returns status code <span style="color: green;">200</span> on successful start.
                            </p>

                            <hr>

                            <ol start="5">
                                <li><b>POST</b> <span style="color: green;">/api/end/{bench_id}</span></li>
                            </ol>

                            <p>
                                <b>Description</b>: Marks the bench as completed.
                            </p>

                            <p>
                                <b>URL Parameter</b>:
                                <ul class="nested-list">
                                    <li><span style="color: green;">bench_id</span>: The identifier of the bench.</li>
                                </ul>
                            </p>

                            <p>
                                <b>Response</b>: Returns the final result as a string upon successful completion.
                            </p>

                            <hr>

                            <p>
                                <b>Batch Input</b>. Each batch contains data such as:

                                <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{
    "sequence": &lt;int&gt;,
    "print_id": &lt;int&gt;,
    "tile_id": &lt;int&gt;,
    "layer": &lt;int&gt;,
    "tif": &lt;binary data&gt;                                        
}</code></pre>
                            </p>

                            <p>
                                Where:
                                <ul class="nested-list">
                                    <li>sequence: sequence number that uniquely identifies a batch;</li>
                                    <li>print_id: identifier of the object being printed;</li>
                                    <li>tile_id: identifier of the tile (see the figure above for reference);</li>
                                    <li>layer: identifier of the layer (z position) of the tile (see the figure above for reference);</li>
                                    <li>tif: the image in tif format.</li>
                                </ul>
                            </p>

                            <p>
                                <b>Response</b>. Each response contains data such as:

                                <pre style="margin: 0; padding: 0;"><code style="margin: 0; padding: 0;">{
    "sequence": &lt;int&gt;,
    “query”: 0
    "print_id": &lt;int&gt;,
    "tile_id": &lt;int&gt;,
    "saturated": &lt;int&gt;,
    "centroids": [C0, C1, …]                                   
}</code></pre>
                            </p>

                            <p>
                                Where:
                                <ul class="nested-list">
                                    <li>sequence: sequence number that uniquely identifies a batch;</li>
                                    <li>print_id: identifier of the object being printed;</li>
                                    <li>tile_id: identifier of the tile;</li>
                                    <li>saturated: number of saturated points;</li>
                                    <li>centroids: list of centroids, where each centroid is defined by its <i>coordinates</i> (x, y) and its <i>size</i> (the number of points belonging to its cluster)</li>
                                </ul>
                            </p>

                        </div>
                        <div>

                            <h1>Awards and Selection Process</h1>

                            <p>
                                Participants of the challenge compete for the performance award. The winner is determined through the automated evaluation platform Challenger (see above), according to criteria of speed (throughput and latency). In the case of submissions with similar performance characteristics, the committees will evaluate qualities of the solutions that are not tied to performance. Specifically, participants are encouraged to consider the practicability of the solution.
                            </p>
                            <p>
                                Regarding the practicability of a solution, we encourage participants to push beyond solving a problem correctly only in the functional sense. Instead, the aim is to provide a reusable and extensible solution that is of value beyond this year’s Grand Challenge alone, using available software systems or home-made but general-purpose processing engines.
                            </p>
                            <p>
                                In particular, we will reward those solutions that adhere to a list of non-functional criteria, such as configurability, scalability (horizontal scalability is preferred), operational reliability/resilience, accessibility of the solution's source code, integration with standard (tools/protocols), documentation, security mechanisms, deployment support, portability/maintainability, and support of special hardware (e.g., FPGAs, GPUs, SDNs, ...). These criteria are driven by industry use cases and scenarios so that we make sure solutions are applicable in practical settings.
                            </p>
                            
                        </div>
                        <div>

                            <h1>Important Dates</h1>
                            <!-- <li>
                                Release of the challenge, initial data set: <b>November 6th, 2023</b>
                            </li> -->
                            <li>
                                Registration and download of local test environment: <b>December, 2024</b>
                            </li>
                            <li>
                                Online evaluation platform opens: <b>February, 2025</b>
                            </li>
                            <li>
                                Deadline for short paper (max 6 pages) with design/implementation details (notification follows fast review): <b>May 2nd, 2025</b>
                            </li>
                            <li>
                                Deadline for Camera Ready submission: <b>May 9th, 2025</b>
                            </li>
                            <li>
                                Online evaluation platform closes: <b>May 9th, 2025</b>
                            </li>
                        </div>

                        <div>
                            {% if site.data.chairs.grand_challenge_chairs %}
                            <h1>Grand Challenge Co-Chairs</h1>
                            {% for chair in site.data.chairs.grand_challenge_chairs %}
                            <li>{{ chair.firstname }} {{ chair.lastname }}, {{ chair.institution }}</li>
                            {% endfor %}
                            {% endif %}
                        </div>

                        <div>
                            <h1>ACM Policies and Procedures</h1>
                           <p>
                               By submitting your article to an ACM Publication, you are hereby acknowledging that you 
                               and your co-authors are subject to all <a href="https://www.acm.org/publications/policies">ACM Publications Policies</a>, 
                               including <a href="https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects">ACM's 
                                   Publications Policy on Research Involving Human Participants and Subjects</a>. ACM will investigate alleged 
                               violations of this policy or any ACM Publications Policy and may result in a full retraction 
                               of your paper, in addition to other potential penalties, as per ACM Publications Policy.
                           </p>
                           <p>
                               Regarding <a href="https://www.acm.org/publications/policies/new-acm-policy-on-authorship">ACM Policy on Authorship</a>, 
                               which also provides policies on the use of generative AI tools in preparing manuscripts, please check the 
                               <a href="https://www.acm.org/publications/policies/frequently-asked-questions">frequently asked questions</a> page.
                           </p>
                           <p class="justfy">
                               Please ensure that you and your co-authors obtain <a href="https://authors.acm.org/author-resources/orcid-faqs">an ORCID ID</a>, 
                               so you can complete the publishing process for your accepted paper.
                           </p>
                       </div>

                    </div>
                </div>

                {% include side.html %}
            </div>
    </section>

    {% include footer.html %}
</body>
</html>